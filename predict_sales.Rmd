---
title: 'Walmart Recruiting II: Sales in Stormy Weather'
author: ''
date: ''
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
    toc_theme: readable
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, include = TRUE, message = FALSE, warning = FALSE)
```


# Abstract

This project aims to design a fit using linear models to predict the sales of weather-sensitive Walmart products that are affected by snow and rain. By using Kaggle's website, we are able to submit a CSV file that outputs a score. Lower scores indicate that the fit of the models on the data is very good. 

# Introduction

This project investigates items sold in Walmart stores and some of the weather factors that affect the sales of these items. The datasets used for this report come from Kaggle, an online community for analytical driven learners to compete. The intent of this report is to be able to accurately predict how major weather events will impact the sales of various items in Walmart. This is useful for store managers to prepare adequately for what shoppers will or will not need, pre and post storms. 

The Walmart data contains the sales of 111 items from 45 different stores that are potentially impacted by weather events. While there are 45 stores, there are only 20 weather stations so some of the stores are under the same weather station. 

The variables, including the response in the dataset are:

- `store_nbr` a id variable that represents one of the 45 stores
- `station_nbr` a id variable that represents one of the 20 weather stations
- `item_nbr` - a id variable that represents one of the 111 items
- `id` - a unique id variable that represents store_nbr, item_nbr, and date
- `date` - the day of a weather event or sales
- `units` - the amount of units sold of an item for any selected day

The goal of this project is to investigate the variables in the dataset and build a model that will accurately predict the sales of these items.

# Plan for Analysis

To begin the project, we will start by cleaning the data as well as analyzing relationships between variables. To analyze these relationships, we will look at correlation matrices and plots. The purpose of this is to identify if any linear regression assumptions are being violated such as multicollinearity, when two or more variables are strongly dependent. After doing this exploratory data analysis, we will begin to build a linear model that will allow us to get an initial Kaggle score for the Walmart dataset. After using this linear model to get an Initial Kaggle score, we plan to use different variable selection methods to find a better model that will give us a higher Kaggle score. 


# Exploratory Data Analysis

```{r, messages = FALSE, warning=FALSE}
library(ggplot2)
library(plyr)
library(tidyverse)
```


```{r, warning=FALSE, message=FALSE}
newdata <- read.csv("~/Desktop/stat425_final/weather.csv")
newdata$snowfall <- revalue(newdata$snowfall, c("M"=0)) 
newdata$preciptotal <- revalue(newdata$preciptotal, c("M"=0)) 
newdata$preciptotal <- revalue(newdata$preciptotal, c("T"=0)) 
newdata$snowfall = as.numeric(as.character(newdata$snowfall))
newdata$preciptotal = as.numeric(as.character(newdata$preciptotal))

newdata$stnpressure =as.numeric(newdata$stnpressure)
newdata$tmax =as.numeric(newdata$tmax)
newdata$tmin =as.numeric(newdata$tmin)
newdata$tmin =as.numeric(newdata$tmin)

newdata$dewpoint =as.numeric(newdata$dewpoint)
newdata$wetbulb =as.numeric(newdata$wetbulb)
newdata$heat =as.numeric(newdata$heat)
newdata$cool =as.numeric(newdata$cool)
newdata$sunrise =as.numeric(newdata$sunrise)
newdata$sealevel =as.numeric(newdata$sealevel)
newdata$resultspeed =as.numeric(newdata$resultspeed)
newdata$resultdir =as.numeric(newdata$resultdir)
newdata$avgspeed =as.numeric(newdata$avgspeed)
newdata$sunset =as.numeric(newdata$sunset)
```


```{r}
library(ggcorrplot)

corr_weather = cor(select(newdata,tmax, tmin, dewpoint, stnpressure))
ggcorrplot(corr_weather,lab=TRUE, colors = c("maroon", "white", "purple"))

corr_weather = cor(select(newdata,wetbulb,heat,cool,sunrise))
ggcorrplot(corr_weather,lab=TRUE, colors = c("pink", "white", "lightblue"))

corr_weather = cor(select(newdata,sunrise, sealevel,resultspeed,resultdir,avgspeed,sunset))
ggcorrplot(corr_weather,lab=TRUE, colors = c("pink", "white", "maroon"))
```


One part of our exploratory data analysis was to look at the variables in correlation matrices. Correlation matrices are helpful when looking at variables because we are able to see which variables relate to other variables in the dataset. Looking at our first correlation matrix, we can see there is a strong, positive correlation between dewpoint and the variables tmax and tmin.  We decided to take out tavg (average temperature) because we have the variables tmax and tmin, and the average temperature will have a similar correlation to the other variables as tmax and tmin. 

In our second correlationship matrix, we are able to see the correlation between the variables wetbulb, heat, cool, and sunrise. Sunrise did not really have a very strong correlation with any of the variables in this matrix. One interesting observation is that the variable wetbulb had a strong, negative correlation with heat and a strong, positive correlation with cool. The wet-bulb termperature is the temperature that is read by a thermometer covered in a water-soaked cloth. Wet bulb termperature has a lot to do with the measurement with humidity.

In our third correlation matrix, we are looking at the variables sunrise, sealevel, resultspeed, resultdir, avgspeed, and sunset. Sunrise does not have a very strong correlation with any of the variables in this matrix except for sunset, which is to be expected. Sea level also does not have a strong correlation with any of the other variables. One observation that stood out was that the precipitation total had a negative correlation with sunrise and a positive relationship with sunset. 

```{r}
plot(newdata$wetbulb ~ newdata$preciptotal, col = "blue", main = "Humidity Vs Rainfall", xlab = "Precipitation", ylab = "Wetbulb (Measurement of Humidity)")
```

By looking at this graph, we can see a relationship between precipitation and wetbulb, which is a measurement of humidity. As precipitation increases, so does wetbulb. This makes sense because as precipitation increases, the humidity in the air will increase. When it rains, it will increase the relative humidity because the air will draw more water. 

```{r}
plot(newdata$preciptotal ~ newdata$tmax, main = "Precipitation based on Maximum Temperature", xlab = "Maximum Temperature", ylab = "Total Precipitation", col = "DarkRed")
```
This graph is looking at a day's maximum temperature versus total precipitation. We found this plot interesting because it follows a skewed normal distribution with a mean of about 70 degrees. At lower temperatures, there usually is a lower total precipitation. At high temperatures, like 100 degrees, the total precipitation was also averagely low. When we take a look at a day with a very average temperature, anywhere from about 65-85 degrees, we can see that there is a much higher variation in total precipitation. People are probably less likely to come to a store if it is pouring rain, which seems to occur more often when the temperature is about average. 

# Linear Regression

```{r include=FALSE}
key <- read.csv("~/Desktop/stat425_final/key.csv")
train <- read.csv("~/Desktop/stat425_final/train.csv")
test <- read.csv("~/Desktop/stat425_final/test.csv")
weather <- as.data.frame(read.csv(file="~/Desktop/stat425_final/weather.csv", header=TRUE, sep=","))
```

```{r message=FALSE, warning=FALSE, results="hide"}
key_weather <- merge(key , weather,by="station_nbr")
key_weather[order(as.Date(key_weather$date, format="%d/%m/%Y")),]
key_weather$snowfall <- revalue(key_weather$snowfall, c("M"=0)) 
key_weather$preciptotal <- revalue(key_weather$preciptotal, c("M"=0)) 
key_weather$preciptotal <- revalue(key_weather$preciptotal, c("T"=0)) 
key_weather$snowfall = as.numeric(as.character(key_weather$snowfall))
key_weather$preciptotal = as.numeric(as.character(key_weather$preciptotal))
```

We revalued M and T in snowfall and preciptotal in order to make all values numeric.

```{r,message=FALSE, warning=FALSE}
key_weather$stnpressure =as.numeric(key_weather$stnpressure)
key_weather$tmax =as.numeric(key_weather$tmax)
key_weather$tmin =as.numeric(key_weather$tmin)

key_weather$dewpoint =as.numeric(key_weather$dewpoint)
key_weather$wetbulb =as.numeric(key_weather$wetbulb)
key_weather$heat =as.numeric(key_weather$heat)
key_weather$cool =as.numeric(key_weather$cool)

key_weather$resultspeed =as.numeric(key_weather$resultspeed)
key_weather$resultdir =as.numeric(key_weather$resultdir)
key_weather$avgspeed =as.numeric(key_weather$avgspeed)


key_weather$tavg =as.numeric(key_weather$tavg)
key_weather$sunrise =as.numeric(key_weather$sunrise)
key_weather$sunset =as.numeric(key_weather$sunset)
```

The data in key_weather were all factors and we changed it to a numeric class so that we could do regressoin and further anaylsis.

```{r,message=FALSE, warning=FALSE}
key_weather=key_weather %>% mutate(tmax=ifelse(is.na(tmax), median(tmax, na.rm=TRUE), tmax))
key_weather=key_weather %>% mutate(tmin=ifelse(is.na(tmin), median(tmin, na.rm=TRUE), tmin))
key_weather=key_weather %>% mutate(tavg=ifelse(is.na(tavg), median(tavg, na.rm=TRUE), tavg))
key_weather=key_weather %>% mutate(depart=ifelse(is.na(depart), median(depart, na.rm=TRUE), depart))
key_weather=key_weather %>% mutate(dewpoint=ifelse(is.na(dewpoint), median(dewpoint, na.rm=TRUE), dewpoint))
key_weather=key_weather %>% mutate(wetbulb=ifelse(is.na(wetbulb), median(wetbulb, na.rm=TRUE), wetbulb))
key_weather=key_weather %>% mutate(heat=ifelse(is.na(heat), median(heat, na.rm=TRUE), heat))
key_weather=key_weather %>% mutate(cool=ifelse(is.na(cool), median(cool, na.rm=TRUE), cool))
key_weather=key_weather %>% mutate(sunrise=ifelse(is.na(sunrise), median(sunrise, na.rm=TRUE), sunrise))
key_weather=key_weather %>% mutate(sunset=ifelse(is.na(sunset), median(heat, na.rm=TRUE), sunset))
key_weather=key_weather %>% mutate(snowfall=ifelse(is.na(snowfall), median(snowfall, na.rm=TRUE), snowfall))
key_weather=key_weather %>% mutate(preciptotal=ifelse(is.na(preciptotal), median(preciptotal, na.rm=TRUE), preciptotal))
key_weather=key_weather %>% mutate(stnpressure=ifelse(is.na(stnpressure), median(stnpressure, na.rm=TRUE), stnpressure))
key_weather=key_weather %>% mutate(sealevel=ifelse(is.na(sealevel), median(sealevel, na.rm=TRUE), sealevel))
key_weather=key_weather %>% mutate(resultspeed=ifelse(is.na(resultspeed), median(resultspeed, na.rm=TRUE), resultspeed))
key_weather=key_weather %>% mutate(resultdir=ifelse(is.na(resultdir), median(resultdir, na.rm=TRUE), resultdir))
key_weather=key_weather %>% mutate(avgspeed=ifelse(is.na(avgspeed), median(avgspeed, na.rm=TRUE), avgspeed))
```

We replaced the NAs in the data with the median of the variable in order to get to the true value of the data. We chose this option over na.omit becuase we did not want to get rid of data.

```{r,message=FALSE, warning=FALSE}
final_df <- merge(key_weather,train,by=c("store_nbr","date"))
final_test <- merge(key_weather,test,by=c("store_nbr","date"))
```

We used the merge function in r to create two dataframes by merging the key_weather data with the training and testing data. We used store_nbr and date to merge the two datasets.

```{r,message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
final_test = final_test %>% unite(id, store_nbr, item_nbr, date, sep = "_", remove = FALSE)
```

We added a row to the final test data frame so that we could create unique IDs in order to submit to Kaggle.

```{r,message=FALSE, warning=FALSE}
library(MASS)
# fit SLR
model1 = lm(units ~ store_nbr + dewpoint + wetbulb + heat +cool +snowfall +preciptotal + stnpressure + resultspeed + resultdir + avgspeed + item_nbr + tmin +tmax, data = final_df)
summary(model1)
```  

Looking at the summary of model 1, we can see that dewpoint, wetbulb, preciptotal, avgspeed, and tmin all were not very significant to the model. Because of this, we will further analyze model 1 to see if we should take out any of these coefficients.

```{r,message=FALSE, warning=FALSE}
library(car)
vif(model1)
```

When looking at the VIFs in model 1, we can see that wetbulb an tmin both had a value higher than 5. This means there is an issue with these coefficients having collinearity. Because of this, we decided to take out these coefficients in our model 2.

```{r,message=FALSE, warning=FALSE}
model2 = lm(units ~ store_nbr + dewpoint  + heat +cool +snowfall +preciptotal + stnpressure + resultspeed + resultdir + avgspeed + item_nbr +tmax, data = final_df) #took out wetbulb and tmin
summary(model2)
vif(model2)
```

Looking at the summary of model 2, we can see that many of the coefficients are significant, but some are not. For example, dewpoint, preciptotal, and resultdir were not very significant to the model.

Because none of the VIFs were above 5, none of the coefficients in model 2 have an issue with collinearity.

Model 2 is going to be the first model that we test.

```{r,message=FALSE, warning=FALSE}
confint(model2)
```

These are the confidence intervals for the coefficient estimates in model 2.

```{r,message=FALSE, warning=FALSE}
anova(model2)
```

Because the p-value of the coefficients preciptotal and avgspeed were both very high (not significant), we took these coefficients out of model 2. Many of the other variables p-values were low, meaning they are significant to the model. We removed values that had high p-values.

```{r,message=FALSE, warning=FALSE}
par(mfrow = c(2,2))
plot(model2)
```

The normality assumption is mostly correct because the points on the qq plot follow the line closely until the 2nd and 4th quantile, where they deviate slightly.

```{r,message=FALSE, warning=FALSE}
aic_backward = step(model2, trace = 0)
aic_backward
```


```{r,message=FALSE, warning=FALSE}
model_intercept = lm(units ~ 1, data=final_df)
aic_forward = step(model_intercept, scope = list(upper = model1), trace = 0, direction = "forward")
aic_forward
```

```{r,message=FALSE, warning=FALSE}
cv <- function(model) {
   SSE.predict <- numeric(10)
   folds <- rep_len(1:10, nrow(final_df))
   
   for (k in 1:10) {
      test.index <- which(folds == k)
      data.train <- final_df[-test.index, ]
      data.test <- final_df[test.index, ]
      lm.temp <- lm(model, data = data.train)
      SSE.predict[k] <-
         crossprod(predict(lm.temp, data.test) - data.test$units)
   }
   return(sum(SSE.predict))
}
```

```{r,message=FALSE, warning=FALSE}
cv(aic_backward)
```

```{r,message=FALSE, warning=FALSE}
cv(aic_forward)
```


Our initial Kaggle score is 1.04 which we got by running model 2. 



# Improvements

## Forwards BIC

To start with our improvement section, we tried doing a forwards BIC model.

```{r,message=FALSE, warning=FALSE}
n = length(resid(model2))
mod2_start <- model2
model2_forw_bic = step(
  mod2_start,
  scope = units ~ store_nbr + dewpoint + heat + cool + snowfall + 
    preciptotal + stnpressure + resultspeed + resultdir + avgspeed + 
    item_nbr + tmax,
  direction = "forward", k = log(n), trace = 0
)
```
```{r}
summary(model2_forw_bic)
```

In this model, we used forwards BIC for variable selection. By submitting this to Kaggle, we recieved a score of 1.04519. This is exactly the same as the model before. Because of this, we are going to try more ways to improve our Kaggle score. We did not think this model would be the best fit because there are multiple variables that are not significant. Dewpoint, preciptotal,and avg speed all had a p-value above .1, and cool had a high p-value as well.


## Backwards BIC

```{r,message=FALSE, warning=FALSE}
n = length(resid(model2))
model2_back_bic = step(model2, direction = "backward", k = log(n), trace = 0)
```
```{r}
summary(model2_back_bic)
```

When we ran the summary for the backwards BIC, we were pretty happy with the results and thought that this would lead to a lower Kaggle score than before. This model output a Kaggle score of 0.82893. This is the best Kaggle score out of any of our models. 

## Stepwise BIC

```{r,message=FALSE, warning=FALSE}
mod2_both_bic = step(
  mod2_start,
  scope = units ~ store_nbr + dewpoint + heat + cool + snowfall + 
    preciptotal + stnpressure + resultspeed + resultdir + avgspeed + 
    item_nbr + tmax,
  direction = "both", k = log(n), trace = 0
)
```

```{r}
summary(mod2_both_bic)
```

This model, based on both backwards and forwards BIC, gave us the same linear model, which led to the same Kaggle score of 0.82893. This model is the same as the backwards BIC model, and it gave us the same Kaggle score as the backwards BIC model.



## Stepwise AIC

```{r,message=FALSE, warning=FALSE}
mod2_both_aic = step(
  mod2_start,
  scope = units ~ store_nbr + dewpoint + heat + cool + snowfall + 
    preciptotal + stnpressure + resultspeed + resultdir + avgspeed + 
    item_nbr + tmax,
  direction = "both", trace = 0
)
```

```{r}

model6 <- mod2_both_aic
summary(model6)
```

Using Stepwise AIC, we got a Kaggle score 1.04159. In this model, there were 9 coefficients, with all of them being somewhat significant. 7 of the variables were very significant, which is a good sign that the model is fit pretty decently.

## Backwards AIC
```{r,message=FALSE, warning=FALSE}
mod2_back_aic = step(mod2_start, direction = "backward", trace = 0)

summary(mod2_back_aic)
```
```{r}
model7 <- mod2_back_aic
```

Our Kaggle score for this model was 1.04519. This model also had 9 coefficients, with all of them being somewhat significant. The Kaggle score was not improved though.

## Forward AIC

```{r,message=FALSE, warning=FALSE}
mod2_forw_aic = step(
  mod2_start,
  scope = units ~ store_nbr + dewpoint + heat + cool + snowfall + 
    preciptotal + stnpressure + resultspeed + resultdir + avgspeed + 
    item_nbr + tmax,
  direction = "forward", trace = 0
)
```

```{r}
summary(mod2_forw_aic)
model8 <- mod2_forw_aic
```

The Kaggle score for this model was 1.04519. This model had a total of 12 coefficients, with some of them not being significant. This was an indicator that it would probably not increase our Kaggle score.


# Extra Models

```{r}
library(randomForest)

mysample <- final_df[sample(1:nrow(final_df), 100000),]

model1 <- randomForest(units ~ heat   + stnpressure + resultspeed + resultdir + item_nbr +tmax, data = mysample, importance = TRUE)
```
Our Kaggle score for our random forest model is 0.72589.


# Our Final Model
After testing all of our models and getting a Kaggle score, we decided to go with the Backwards BIC model from this section to be our 'best' model. This model had a total of 6 coefficients, all with a p-value of 0 (all of them are significant). 

```{r}
par(mfrow = c(2,2))
plot(model2_back_bic)
```
In the Normal Q-Q plot for our best model, we can see that the data follows closely with the dashed line up until about the fourth quantile, where it starts to curve above the dashed line. In the Residuals vs Leverage plot, there seems to be no outliers above the dashed line of .5.


```{r}
vif(model2_back_bic)
```
Looking at the VIFs of our best model, we can see that none of the coefficients have an issue with collinearity. 

```{r}
anova(model2_back_bic)
```
The anova test on the model shows that all the variables are significant. 

# Conclusion
```{r}
dt <- data.frame("Model Name" = c("Model 2","Forwards BIC","Backwards BIC","Stepwise BIC","Forwards AIC","Backwards AIC","Stepwise AIC","Random Forest"), "Kaggle Score"= c(1.04,1.04519,0.82893,0.82893,1.04519,1.04519,1.04519,0.72589))
dt
```

In this report, we looked at the data to investigate weather sensitive items from Walmart that were impacted by weather factors. To make our best model, we used forward, backward, and stepwise AIC and BIC as variable selection techniques. Our aim was to form a model that fit the data better and gave us a lower Kaggle score. Once we chose the backwards BIC model as our best performing model, we used data visualizations and looked at variation inflation factors to make sure none of our coefficients were collinear. Our final Kaggle score of .82893 was an improvement from our original model, which gave us a score of 1.04.

For our data, we ended up cleaning our data by replacing the NA values with the median value of the variable. The problem we encountered with that was obtaining a median value. However, we went around that issue by ommitting the NA values, getting the median value, and then replacing the values. We then did our exploratory analysis of the variables and made sure that they were not violating any of the assumptions of linear regression. Once we checked for all of our variables, we started building our models. We went through a lot of models because we used different selection techniques such as AIC and BIC forwards, backwards, and stepwise.

In our final model we ended with the variables: store_nbr, snowfall, preciptotal, stn_pressure, resultspeed, item_nbr, and tmax. This makes sense because each of these variables has a significant impact on the weather compared to some of the other variables in the full model such as sunrise and sunset. Our final model has a very strong F statistic and is extremely significant with a p value of 0.



### Code to submit to kaggle 

```{r, echo=TRUE}
test_predictions <- model2 %>% predict(final_test)
x <- data.frame(final_test$id,test_predictions)
write.csv(x, "predict.csv" , col.names = c("id", "units"))
```
